---
slug: building-ai-powered-developer-tools
title: "Building AI-Powered Developer Tools: A Complete Architecture Guide"
excerpt: Deep dive into how we built Plunder Academy's AI Chat Assistant and Contract Reviewer—covering architecture, prompt engineering, model selection, and implementation patterns you can adapt for any use case.
level: advanced
tags:
  - ai
  - architecture
  - development
  - tools
  - tutorial
---

Building effective AI-powered developer tools isn't just about calling an API—it's about thoughtful architecture, careful prompt engineering, and understanding the tradeoffs between different providers and models. This article documents exactly how we built Plunder Academy's AI tooling, with enough detail that you could recreate it or adapt the patterns for your own projects.

## What We Built

Plunder Academy has two AI-powered tools:

1. **AI Chat Assistant** (`/chat`): A conversational interface for developers learning Solidity and EVM development. It maintains context across messages, recommends relevant training modules, and provides code examples with explanations.

2. **AI Contract Reviewer** (`/reviewer`): A specialized security auditor that analyzes Solidity code, identifies vulnerabilities, provides severity ratings, and outputs structured markdown reports with fix recommendations.

Both tools share similar infrastructure but differ significantly in their prompt engineering, response formats, and frontend implementations.

## Architecture Overview

Our architecture follows a clean separation of concerns:

<table>
<thead>
<tr>
<th>Layer</th>
<th>Technology</th>
<th>Purpose</th>
</tr>
</thead>
<tbody>
<tr>
<td>Frontend</td>
<td>Next.js + Vercel AI Elements</td>
<td>User interface and streaming display</td>
</tr>
<tr>
<td>Hooks</td>
<td>Vercel AI SDK (@ai-sdk/react)</td>
<td>State management and API calls</td>
</tr>
<tr>
<td>API Routes</td>
<td>Next.js Route Handlers</td>
<td>Server-side processing</td>
</tr>
<tr>
<td>Gateway</td>
<td>Vercel AI Gateway</td>
<td>Provider routing, cost tracking, fallbacks</td>
</tr>
<tr>
<td>Models</td>
<td>OpenAI, Google, Meta, xAI</td>
<td>Actual inference</td>
</tr>
</tbody>
</table>

The flow is straightforward:

<AIArchitectureFlow />

```
User Input → useChat/useCompletion → API Route → AI Gateway → Model → Streaming Response → UI
```

What makes this architecture powerful is the **AI Gateway layer**—it abstracts provider-specific details and enables routing, fallbacks, and cost optimization without changing application code.

## The Vercel AI SDK

The [Vercel AI SDK](https://sdk.vercel.ai) provides the foundation for our implementation. It's framework-agnostic but integrates seamlessly with Next.js.

### Core Concepts

**Server-Side Functions:**
- `streamText()`: Streams text responses token-by-token
- `generateText()`: Returns complete responses (non-streaming)
- `streamObject()`: Streams structured JSON objects
- `generateObject()`: Returns complete structured objects

**Client-Side Hooks:**
- `useChat()`: Manages conversational state with message history
- `useCompletion()`: Handles single prompt/completion interactions
- `useObject()`: Works with streaming structured data

### Why Two Different Hooks?

We use `useChat()` for the chat assistant and `useCompletion()` for the reviewer—this isn't arbitrary:

**`useChat()` for Conversations:**
- Maintains message history automatically
- Handles user/assistant message pairs
- Supports conversation IDs for persistence
- Perfect for back-and-forth dialogue

**`useCompletion()` for Single Tasks:**
- Simple prompt → response pattern
- No conversation state needed
- Ideal for one-shot tasks like code review
- Cleaner API for task-focused tools

Here's how they differ in practice:

```typescript
// Chat: Conversational, maintains history
const { messages, sendMessage, status } = useChat({
  id: conversationId,  // Enables persistence
});

// Send a message that adds to history
sendMessage({ text: "Explain reentrancy attacks" });

// Reviewer: Single prompt, no history
const { completion, complete, isLoading } = useCompletion({
  api: "/api/reviewer",
  streamProtocol: "text",
});

// Submit code for review (no history)
await complete(solidityCode);
```

## API Route Implementation

Both tools follow a similar server-side pattern. Here's the essential structure:

```typescript
import { streamText, convertToModelMessages } from "ai";
import { readFileSync } from "fs";
import { join } from "path";

export const runtime = "nodejs";
export const maxDuration = 60; // Reviewer needs more time

const DEFAULT_MODEL = "openai/gpt-oss-120b";

// Load system prompt from markdown file
function getSystemPrompt(): string {
  const promptPath = join(process.cwd(), 'src/app/api/reviewer/reviewer-system-prompt.md');
  return readFileSync(promptPath, 'utf-8').replace(/^# .*\n\n/, '');
}

export async function POST(req: Request) {
  const body = await req.json();
  const prompt = body.prompt?.trim();
  
  const result = await streamText({
    model: DEFAULT_MODEL,  // AI Gateway model identifier
    system: getSystemPrompt(),
    prompt,
    temperature: 0.2,  // Lower = more consistent
    providerOptions: {
      gateway: {
        only: ['cerebras'],  // Prefer fast inference
      },
    },
  });

  return result.toTextStreamResponse();
}
```

### Key Implementation Details

**1. Runtime and Duration:**
```typescript
export const runtime = "nodejs";
export const maxDuration = 60;
```
The chat uses 30 seconds; the reviewer uses 60 seconds. Security audits are more complex and generate longer responses.

**2. System Prompts from Markdown:**
We store prompts in separate `.md` files rather than inline strings. This provides:
- Version control for prompt iterations
- Easier editing without touching code
- Clear separation of concerns
- Ability to use markdown formatting in prompts

**3. Model Format:**
The AI Gateway uses `provider/model-name` format (e.g., `openai/gpt-oss-120b`, `google/gemini-2.5-flash-lite`). This abstracts the underlying provider.

**4. Provider Options:**
```typescript
providerOptions: {
  gateway: {
    only: ['cerebras'],  // Prefer this inference provider
  },
},
```
You can route to specific inference providers (Cerebras, Groq, Together, etc.) for speed or cost optimization.

## AI Gateway: The Routing Layer

The AI Gateway is what makes our architecture flexible. Instead of calling OpenAI directly, we route through a gateway that can:

- **Route to optimal providers**: Same model, different inference backends
- **Handle fallbacks**: If one provider fails, try another
- **Track costs**: Unified cost reporting across providers
- **Enable caching**: Response caching for repeated queries
- **Provide observability**: Centralized logging and metrics

### Provider Metadata

The gateway returns rich metadata we log for debugging:

```typescript
result.providerMetadata?.then((metadata) => {
  if (metadata?.gateway?.routing) {
    console.log({
      originalModelId: routing.originalModelId,
      resolvedProvider: routing.resolvedProvider,
      finalProvider: routing.finalProvider,
      cost: metadata.gateway.cost,
      responseTime: routing.attempts?.[0]?.endTime - routing.attempts?.[0]?.startTime
    });
  }
});
```

This tells us exactly which provider served the request, how long it took, and what it cost.

### Alternative Gateway Options

**If you're not using Vercel, consider:**

<table>
<thead>
<tr>
<th>Provider</th>
<th>Strengths</th>
<th>Best For</th>
</tr>
</thead>
<tbody>
<tr>
<td>Vercel AI Gateway</td>
<td>Seamless Vercel integration, AI SDK native</td>
<td>Vercel deployments</td>
</tr>
<tr>
<td>Cloudflare AI Gateway</td>
<td>Edge caching, rate limiting, analytics</td>
<td>Cloudflare ecosystem, global edge</td>
</tr>
<tr>
<td>OpenRouter</td>
<td>Largest model selection, unified pricing</td>
<td>Model experimentation, cost optimization</td>
</tr>
<tr>
<td>LiteLLM</td>
<td>Self-hosted, open source</td>
<td>Privacy, custom deployments</td>
</tr>
</tbody>
</table>

**Cloudflare AI Gateway Setup:**
```typescript
// With Cloudflare's gateway
const response = await fetch(
  `https://gateway.ai.cloudflare.com/v1/{account_id}/{gateway_slug}/openai/chat/completions`,
  {
    method: 'POST',
    headers: {
      'Authorization': `Bearer ${OPENAI_API_KEY}`,
      'Content-Type': 'application/json',
    },
    body: JSON.stringify({
      model: 'gpt-4o-mini',
      messages: [{ role: 'user', content: prompt }],
      stream: true,
    }),
  }
);
```

**OpenRouter Setup:**
```typescript
// With OpenRouter for model variety
const response = await fetch('https://openrouter.ai/api/v1/chat/completions', {
  method: 'POST',
  headers: {
    'Authorization': `Bearer ${OPENROUTER_API_KEY}`,
    'Content-Type': 'application/json',
  },
  body: JSON.stringify({
    model: 'anthropic/claude-3.5-sonnet',  // or any supported model
    messages: [{ role: 'user', content: prompt }],
    stream: true,
  }),
});
```

## Model Selection Philosophy

We evaluated dozens of models before settling on our current choices. Here's our evaluation framework:

### Evaluation Criteria

1. **Response Quality**: Does it understand the domain?
2. **Speed**: Time to first token and tokens/second
3. **Cost**: Input/output token pricing
4. **Consistency**: Same quality across different prompts
5. **Format Compliance**: Can it follow structured output requirements?

### Our Model Evaluations

```typescript
// Our testing notes (from actual code comments):
// - openai/gpt-4o-mini ($0.15/$0.60) - ok but slower
// - openai/gpt-5-mini ($0.25/$2) - nice but really slow
// - openai/gpt-5-nano ($0.05/$0.40) - ok but slower
// - openai/gpt-oss-120b ($0.15/$0.75) - really nice and fast on cerebras ✓
// - google/gemini-2.5-flash-lite ($0.10/$0.40) - REALLY like this one ✓
// - meta/llama-4-maverick ($0.20/$0.60) - ok
// - xai/grok-3-mini ($0.30/$0.50) - ok
```

**Our winners:**
- **Primary**: `openai/gpt-oss-120b` via Cerebras (fast inference)
- **Alternative**: `google/gemini-2.5-flash-lite` (great quality/cost ratio)

### Why Inference Provider Matters

The same model can have vastly different performance depending on who's running inference:

<InferenceSpeedVisualizer />

<table>
<thead>
<tr>
<th>Model</th>
<th>Provider</th>
<th>Speed</th>
<th>Notes</th>
</tr>
</thead>
<tbody>
<tr>
<td>Llama 70B</td>
<td>Together</td>
<td>~50 tok/s</td>
<td>Standard GPU</td>
</tr>
<tr>
<td>Llama 70B</td>
<td>Groq</td>
<td>~300 tok/s</td>
<td>Custom LPU chips</td>
</tr>
<tr>
<td>Llama 70B</td>
<td>Cerebras</td>
<td>~500 tok/s</td>
<td>Wafer-scale engine</td>
</tr>
</tbody>
</table>

This is why we use `providerOptions: { gateway: { only: ['cerebras'] } }`—same model, 10x faster.

## System Prompt Engineering

This is where the magic happens. A generic LLM becomes a specialized tool through careful prompt engineering.

### Chat Assistant Prompt Structure

Our chat prompt (~430 lines) follows this structure:

```markdown
# Role Definition
You are a specialized blockchain training assistant...

# Core Focus Areas
- Zilliqa 2.0 EVM development
- Solidity smart contract development
- Training module recommendations

# Domain Knowledge
## Network Details
- Chain IDs, RPC endpoints, gas pricing
## Technical Specifications
- EVM version requirements, address formats

# Training Module Reference Map
## When to recommend which modules
- User asks about X → recommend Y module

# Response Guidelines
## Length Strategy
- Simple queries: 2-3 sentences
- Code requests: minimal working examples
- Complex topics: comprehensive explanations

# Troubleshooting Guide
## Common errors and solutions
```

**Key Principles:**

1. **Role Definition First**: Set the persona before anything else
2. **Explicit Focus Areas**: Tell it what to care about
3. **Domain Knowledge Injection**: Include facts it might not know
4. **Response Length Guidance**: Prevent over-verbose answers
5. **Action Patterns**: When X, do Y

### Reviewer Prompt Structure

The reviewer prompt (~540 lines) is more complex because it must produce structured output:

```markdown
# Role Definition
You are a blockchain security expert...

# Audit Methodology
## Phase 1: Contract Scan
- What to look for, how to flag issues

## Phase 2: Vulnerability Analysis
- Severity scoring framework
- Exploitability verification

## Phase 3: Secure Implementation
- Fix patterns, testing recommendations

# Critical Vulnerability Categories
## Prioritized by severity (9-10, 7-8.9, 5-6.9, etc.)

# MANDATORY OUTPUT FORMAT
## Exact section structure required
## Markdown hierarchy rules
## Concrete examples to follow

# Severity Calibration
## Common mistakes to avoid
## When NOT to mark as critical
```

### The Critical Insight: Exploitability Verification

Early versions of our reviewer flagged everything as "Critical." We fixed this with explicit calibration:

```markdown
**CRITICAL RULE: Before assigning severity above Medium (6+), verify:**
- Can an attacker profit from this specific vulnerability?
- Does it allow stealing funds they don't have permission to access?
- Is there a concrete attack path that results in financial loss?
- If NO → Lower severity and classify as best practice
```

We also include **concrete examples** of correct vs incorrect severity ratings:

```markdown
**CRITICAL Reentrancy (9-10/10) - Exploitable:**
[code example where attacker can steal others' funds]
Why Critical: Attacker drains contract beyond their deposit

**LOW Severity Reentrancy (3-4/10) - Best Practice:**
[code example where only owner can withdraw their own funds]
Why Low: Owner already has permission, re-entry doesn't give more
```

### Output Format Enforcement

For the reviewer, we need consistent markdown structure. We enforce this with:

1. **Explicit section requirements**: "EXACTLY 5 sections in this order"
2. **Heading level rules**: "H2 for sections, H3 for vulnerabilities, H4 for sub-sections"
3. **Concrete examples**: Full example output the model should emulate
4. **Format validation**: We validate output server-side

```markdown
**Your response should look EXACTLY like this:**

## Executive Summary
- Total issues found: [NUMBER]
- Critical: [NUMBER] | High: [NUMBER] | Medium: [NUMBER] | Low: [NUMBER]
- Average Severity Score: [X.X/10]

## Critical Findings
### 1. [VULNERABILITY_TYPE] in `[FUNCTION_NAME]()` function
...
```

## Frontend Implementation

### AI Elements: Pre-Built Components

We use Vercel's [AI Elements](https://github.com/vercel/ai-elements) library for consistent UI patterns:

```typescript
import {
  Conversation,
  ConversationContent,
  ConversationScrollButton,
} from "@/components/ai-elements/conversation";
import {
  Message,
  MessageContent,
  MessageAvatar,
} from "@/components/ai-elements/message";
import {
  PromptInput,
  PromptInputTextarea,
  PromptInputSubmit,
} from "@/components/ai-elements/prompt-input";
import { Response } from "@/components/ai-elements/response";
```

These components handle:
- **Auto-scrolling**: Stay at bottom as responses stream
- **Message formatting**: Consistent user/assistant styling
- **Input handling**: Textarea with submit button
- **Markdown rendering**: Via Streamdown for streaming markdown

### Streaming Markdown Rendering

The `Response` component uses [Streamdown](https://github.com/vercel/streamdown) to render markdown as it streams:

```typescript
import { Streamdown } from 'streamdown';

export const Response = memo(
  ({ className, ...props }: ResponseProps) => (
    <Streamdown
      className={cn(
        'size-full [&>*:first-child]:mt-0 [&>*:last-child]:mb-0',
        className
      )}
      {...props}
    />
  ),
  (prevProps, nextProps) => prevProps.children === nextProps.children
);
```

The `memo` with custom comparison prevents unnecessary re-renders during streaming.

### Chat State Management

The chat page manages several pieces of state:

```typescript
// Conversation management
const [currentConversationId, setCurrentConversationId] = useState(() =>
  createConversationId()
);
const [conversations, setConversations] = useState<StoredConversation[]>([]);

// Core chat hook
const { messages, sendMessage, status, setMessages } = useChat({
  id: currentConversationId,
});

// Interaction tracking
const [interactionIds, setInteractionIds] = useState<Record<number, string>>({});
const [messageTiming, setMessageTiming] = useState<Record<number, { startTime: number; inputText: string }>>({});
```

We persist conversations to localStorage, keyed by wallet address:

```typescript
// Auto-save on message changes
useEffect(() => {
  if (messages.length === 0) return;
  
  saveConversation({
    id: currentConversationId,
    title: generateConversationTitle(messages),
    messages,
    walletAddress: address,
    updatedAt: Date.now(),
  });
  
  setConversations(getConversations(address));
}, [messages, currentConversationId, address]);
```

### Reviewer Implementation

The reviewer is simpler—single prompt/response, no history:

```typescript
const { completion, complete, isLoading, error } = useCompletion({
  api: "/api/reviewer",
  streamProtocol: "text",
});

async function onReview() {
  const id = crypto.randomUUID();
  setInteractionId(id);
  setStartTime(Date.now());
  await complete(code.trim());
}
```

The `streamProtocol: "text"` option tells the hook to expect raw text streaming (vs the default data protocol).

## Analytics and Feedback

We track usage and collect feedback to improve the tools:

```typescript
// Track after response completes
trackAIInteraction({
  id: interactionId,
  walletAddress: address,
  toolType: "auditor",
  inputLength: code.length,
  outputLength: completion.length,
  modelUsed: REVIEW_MODEL_ID,
  durationMs: Date.now() - startTime,
  vulnerabilitiesFound: completion.match(/###\s+\d+\./g)?.length ?? 0,
  sessionId,
});
```

For the reviewer, we estimate vulnerability count by counting numbered findings in the output (`### 1.`, `### 2.`, etc.).

## Adapting for Other Use Cases

The patterns we've described can power many different tools. Here's how to adapt them:

### Step 1: Define Your Tool's Purpose

**Questions to answer:**
- Is it conversational or single-task?
- What domain knowledge does it need?
- What output format do you require?
- How should it handle edge cases?

### Step 2: Choose Your Hook

<table>
<thead>
<tr>
<th>Use Case</th>
<th>Hook</th>
<th>Why</th>
</tr>
</thead>
<tbody>
<tr>
<td>Chatbot, Q&A</td>
<td>useChat</td>
<td>Maintains conversation history</td>
</tr>
<tr>
<td>Code review, analysis</td>
<td>useCompletion</td>
<td>Single prompt/response</td>
</tr>
<tr>
<td>Form filling, data extraction</td>
<td>useObject</td>
<td>Returns structured JSON</td>
</tr>
</tbody>
</table>

### Step 3: Engineer Your Prompt

**Template structure:**

```markdown
# Role
You are a [specific role] that [primary function].

# Knowledge
[Domain-specific facts, terminology, constraints]

# Behavior
## When user asks about X
[Specific response pattern]

## Output Format
[Required structure, examples]

# Boundaries
[What NOT to do, out-of-scope topics]
```

### Step 4: Select Your Model

**Selection criteria:**
1. **Task complexity**: Simple tasks → smaller/cheaper models
2. **Response format**: Structured output → models good at following instructions
3. **Speed requirements**: Real-time → fast inference providers
4. **Cost sensitivity**: High volume → optimize for cost

### Step 5: Add Observability

Always log:
- Request ID (for debugging)
- Token usage (for cost tracking)
- Response time (for performance monitoring)
- Error rates (for reliability)

```typescript
const requestId = crypto.randomUUID();
console.log(`[${requestId}] Started with model: ${modelName}`);

result.usage.then(usage => {
  console.log(`[${requestId}] Tokens:`, usage);
});

result.finishReason.then(reason => {
  console.log(`[${requestId}] Finished:`, reason);
});
```

## Example: Building a Documentation Generator

Let's walk through building a new tool—a documentation generator for Solidity contracts.

### 1. Create the System Prompt

```markdown
# Solidity Documentation Generator

You are a technical writer specializing in smart contract documentation.

## Your Task
Generate comprehensive NatSpec documentation for Solidity contracts.

## Output Format
Return documentation in this exact format:
- Contract overview (2-3 sentences)
- Each function with @dev, @param, @return comments
- Events with descriptions
- Security considerations section

## Guidelines
- Be precise and technical
- Include gas considerations where relevant
- Note any access control requirements
- Flag any functions that modify state
```

### 2. Create the API Route

```typescript
// src/app/api/doc-generator/route.ts
import { streamText } from "ai";

const SYSTEM_PROMPT = `...`;  // From step 1

export async function POST(req: Request) {
  const { code } = await req.json();
  
  const result = await streamText({
    model: "openai/gpt-oss-120b",
    system: SYSTEM_PROMPT,
    prompt: `Generate documentation for this contract:\n\n${code}`,
    temperature: 0.3,
  });

  return result.toTextStreamResponse();
}
```

### 3. Create the Frontend

```typescript
"use client";
import { useCompletion } from "@ai-sdk/react";

export default function DocGenerator() {
  const [code, setCode] = useState("");
  const { completion, complete, isLoading } = useCompletion({
    api: "/api/doc-generator",
    streamProtocol: "text",
  });

  return (
    <div>
      <textarea value={code} onChange={e => setCode(e.target.value)} />
      <button onClick={() => complete(code)} disabled={isLoading}>
        Generate Docs
      </button>
      <div className="prose">
        <Response>{completion}</Response>
      </div>
    </div>
  );
}
```

## Alternative Platforms

### Cloudflare Workers AI

If you're in the Cloudflare ecosystem:

```typescript
// Using Cloudflare Workers AI
export default {
  async fetch(request, env) {
    const { prompt } = await request.json();
    
    const response = await env.AI.run('@cf/meta/llama-2-7b-chat-int8', {
      messages: [
        { role: 'system', content: SYSTEM_PROMPT },
        { role: 'user', content: prompt },
      ],
      stream: true,
    });

    return new Response(response, {
      headers: { 'content-type': 'text/event-stream' },
    });
  },
};
```

**Benefits:**
- No cold starts (always-on edge)
- Built-in rate limiting
- Free tier available
- GPU inference at edge

### OpenRouter

For maximum model flexibility:

```typescript
import OpenAI from 'openai';

const openrouter = new OpenAI({
  baseURL: 'https://openrouter.ai/api/v1',
  apiKey: process.env.OPENROUTER_API_KEY,
});

const completion = await openrouter.chat.completions.create({
  model: 'anthropic/claude-3.5-sonnet',  // Or any of 200+ models
  messages: [{ role: 'user', content: prompt }],
  stream: true,
});
```

**Benefits:**
- 200+ models from one API
- Automatic fallbacks
- Unified pricing
- No provider lock-in

### Self-Hosted (Ollama + LiteLLM)

For privacy or cost control:

```typescript
// LiteLLM provides OpenAI-compatible API for local models
import OpenAI from 'openai';

const litellm = new OpenAI({
  baseURL: 'http://localhost:4000',  // LiteLLM proxy
});

const completion = await litellm.chat.completions.create({
  model: 'ollama/llama2',  // Local Ollama model
  messages: [{ role: 'user', content: prompt }],
  stream: true,
});
```

**Benefits:**
- No API costs after setup
- Data never leaves your infrastructure
- Customize/fine-tune models
- Works offline

## Lessons Learned

### 1. Prompt Engineering is Iterative

Our reviewer prompt went through 20+ iterations. Each time we found:
- False positives → Added calibration examples
- Missing vulnerabilities → Expanded category list
- Inconsistent formatting → Added stricter format rules

### 2. Temperature Matters

- **Chat (0.7-1.0)**: More creative, varied responses
- **Reviewer (0.2-0.3)**: Consistent, focused analysis

### 3. Output Format Enforcement

Models follow examples better than rules. Instead of "use H2 for sections," show exactly what H2 should look like in context.

### 4. Speed vs Quality Tradeoff

Faster models (via specialized inference) often match or exceed quality of slower models on standard infrastructure. Don't assume slower = better.

### 5. Monitor Everything

Without logging, you're flying blind. Track:
- Token usage (cost)
- Response times (UX)
- Error rates (reliability)
- User feedback (quality)

## Conclusion

Building AI-powered developer tools is more engineering than magic. The key components are:

1. **Clear architecture**: Separate concerns between frontend, hooks, routes, and gateway
2. **Thoughtful prompt engineering**: Domain knowledge, output format, and behavior rules
3. **Model selection**: Match model capabilities to task requirements
4. **Gateway abstraction**: Enable provider flexibility and cost optimization
5. **Observability**: Log everything for debugging and improvement

The patterns described here power Plunder Academy's AI tools, but they're generalizable. Whether you're building a code reviewer, documentation generator, or domain-specific chatbot, the same principles apply.

**The code is open source.** Explore our implementation:
- Chat route: `src/app/api/chat/route.ts`
- Reviewer route: `src/app/api/reviewer/route.ts`
- Chat prompt: `src/app/api/chat/chat-system-prompt.md`
- Reviewer prompt: `src/app/api/reviewer/reviewer-system-prompt.md`

---

**Ready to build?** Start with [the Vercel AI SDK documentation](https://sdk.vercel.ai) and experiment with different models through [OpenRouter](https://openrouter.ai) or [Cloudflare AI Gateway](https://developers.cloudflare.com/ai-gateway/).

**Questions about our implementation?** Ask the [AI Chat Assistant](/chat)—it knows the codebase!

